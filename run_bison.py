# coding=utf-8
# Copyright (c) 2019 NVIDIA CORPORATION. All rights reserved.
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""BERT finetuning runner."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time
import re
import shutil
import math
import collections
import csv
import os
from modeling import modeling_reuse_share_embedding_weight_attention
from utils import tokenization
import tensorflow as tf
import horovod.tensorflow as hvd
import time
from utils.utils import LogEvalRunHook, LogTrainRunHook
from utils.preprocessor import *
import numpy as np
from tensorflow.python.client import device_lib
from tensorflow.python.summary.writer import writer_cache
import logging
import sys
import l1fidelity as lf

flags = tf.flags

FLAGS = flags.FLAGS

tf.logging.set_verbosity(logging.INFO)
#handlers = [
#    logging.FileHandler('log'),
#    logging.StreamHandler(sys.stdout)
#]
#logging.getLogger('tensorflow').handlers = handlers

# Required parameters
flags.DEFINE_string(
    "data_dir", None,
    "The input data dir. Should contain the .tsv files (or other data files) "
    "for the task.")

flags.DEFINE_string(
    "query_bert_config_file", None,
    "The config json file corresponding to the pre-trained BERT model. "
    "This specifies the model architecture.")

flags.DEFINE_string(
    "meta_bert_config_file", None,
    "The config json file corresponding to the pre-trained BERT model. "
    "This specifies the model architecture.")

flags.DEFINE_string(
    "ideal_path", "ideal.txt",
    "ideal_path ")

flags.DEFINE_string("task_name", None, "The name of the task to train.")

flags.DEFINE_string("vocab_file", None,
                    "The vocabulary file that the BERT model was trained on.")

flags.DEFINE_string("idf_file", None,
                    "The idf file generated by token level that L1 team used.")

flags.DEFINE_string("full_word_idf_file", None,
                    "The idf file generated by full word (not token level)that L1 team used.")

flags.DEFINE_bool("use_full_word_idf", True, "The flag determines if we use full word idf or not")
flags.DEFINE_string(
    "preprocess_train_file_name", "train.tf_record",
    "The preprocess training file name.")
# Parameters For Key Phrase
flags.DEFINE_bool("add_keyphrase", False, "keyphrase in metastream.")
flags.DEFINE_integer("keyphrase_col", -1, "keyphrase_col")
flags.DEFINE_integer(
    "max_seq_length_keyphrase", 40,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")
flags.DEFINE_string(
    "preprocess_eval_file", None,
    "The preprocess fidelity eval file.")

flags.DEFINE_string(
    "preprocess_predict_file", None,
    "The preprocess training file.")
flags.DEFINE_integer(
    "max_seq_length_meta_description", 128,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")

flags.DEFINE_bool("add_meta_description", False, "add meta_description.")

flags.DEFINE_bool("add_language", False, "add language.")
flags.DEFINE_string(
    "predict_file", None,
    "The raw training file.")

flags.DEFINE_string(
    "output_dir", None,
    "The output directory where the model checkpoints will be written.")

flags.DEFINE_string(
    "preprocess_dir", None,
    "The preprocess data directory.")

flags.DEFINE_string(
    "preprocess_eval_dir", None,
    "The preprocess eval data directory.")

flags.DEFINE_string(
    "preprocess_predict_dir", None,
    "The preprocess predict data directory.")

flags.DEFINE_string(
    "eval_file", "AUTCTier012Evaluation.tsv",
    "eval_file. ")


# Other parameters

flags.DEFINE_string(
    "init_checkpoint", None,
    "Initial checkpoint (usually from a pre-trained BERT model).")


flags.DEFINE_bool(
    "do_lower_case", True,
    "Whether to lower case the input text. Should be True for uncased "
    "models and False for cased models.")

flags.DEFINE_integer(
    "max_seq_length_query", 128,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")

flags.DEFINE_integer(
    "max_seq_length_title", 128,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")

flags.DEFINE_integer(
    "max_seq_length_anchor", 128,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")

flags.DEFINE_integer(
    "max_seq_length_url", 128,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")

flags.DEFINE_integer(
    "max_seq_length_click", 128,
    "The maximum total input sequence length after WordPiece tokenization. "
    "Sequences longer than this will be truncated, and sequences shorter "
    "than this will be padded.")

flags.DEFINE_bool("do_train", False, "Whether to run training.")

flags.DEFINE_bool("do_eval", False, "Whether to run eval on the dev set.")

flags.DEFINE_bool("do_predict", False,
                  "Whether to run the model in inference mode on the test set.")

flags.DEFINE_bool("freeze_embedding", False,
                  "Whether to freeze embedding layer.")

flags.DEFINE_integer("train_batch_size", 32, "Total batch size for training.")

flags.DEFINE_integer("eval_batch_size", 32, "Total batch size for eval.")

flags.DEFINE_integer("predict_batch_size", 8, "Total batch size for predict.")

flags.DEFINE_float("learning_rate", 5e-5,
                   "The initial learning rate for Adam.")

flags.DEFINE_bool("use_trt", False, "Whether to use TF-TRT")

flags.DEFINE_bool("query_weight_attention", True, "Whether to enable weight attention for query.")

flags.DEFINE_bool("meta_weight_attention", True, "Whether to enable weight attention for meta.")
flags.DEFINE_float("num_train_epochs", 3.0,
                   "Total number of training epochs to perform.")

flags.DEFINE_float(
    "warmup_proportion", 0.1,
    "Proportion of training to perform linear learning rate warmup for. "
    "E.g., 0.1 = 10% of training.")

flags.DEFINE_integer("save_checkpoints_steps", 1000,
                     "How often to save the model checkpoint.")

flags.DEFINE_integer("iterations_per_loop", 1000,
                     "How many steps to make in each estimator call.")
flags.DEFINE_integer("num_accumulation_steps", 1,
                     "Number of accumulation steps before gradient update"
                     "Global batch size = num_accumulation_steps * train_batch_size")
flags.DEFINE_bool("use_fp16", False,
                  "Whether to use fp32 or fp16 arithmetic on GPU.")

flags.DEFINE_bool("use_xla", False, "Whether to enable XLA JIT compilation.")
flags.DEFINE_bool("horovod", False,
                  "Whether to use Horovod for multi-gpu runs")
flags.DEFINE_bool("use_one_hot_embeddings", False,
                  "Whether to use use_one_hot_embeddings")

flags.DEFINE_integer("compressor_dim", 100, "output vector size.")
flags.DEFINE_float("nce_temperature", 10, "nce_temperature")
flags.DEFINE_float("nce_weight", 0.5, "nce_weight")
flags.DEFINE_string("activation", 'relu', "activation")
flags.DEFINE_integer("src_col", 0, "src_col")
flags.DEFINE_integer("title_col", 1, "title_col")
flags.DEFINE_integer("anchor_col", 2, "anchor_col")
flags.DEFINE_integer("url_col", 3, "url_col")
flags.DEFINE_integer("click_col", 4, "click_col")
flags.DEFINE_integer("label_col", -1, "label_col")

flags.DEFINE_integer("train_line_count", 136490622, "output vector size.")
flags.DEFINE_integer("train_partition_count", 1000,
                     "Total count of training files.")

flags.DEFINE_bool(
    "use_RNN", False,
    "Whether to enable RNN on Meta path "
    "models and False for cased models.")

flags.DEFINE_bool(
    "use_MLP", False,
    "Whether to enable MLP to covert vector size from 768din to 100dim "
    "models and False for cased models.")

flags.DEFINE_bool(
    "verbose_logging", False,
    "If true, all of the warnings related to data processing will be printed. "
    "A number of warnings are expected for a normal SQuAD evaluation.")

flags.DEFINE_bool(
    "Fix_Sim_Weight", True,
    "Whether to fix sim weight and bias to 20 and -10 "
    "models and False for cased models.")

flags.DEFINE_integer(
    "sim_weight", 10,
    "Whether to fix sim weight and bias to 20 and -10 "
    "models and False for cased models.")

flags.DEFINE_integer(
    "sim_bias", -5,
    "Whether to fix sim weight and bias to 20 and -10 "
    "models and False for cased models.")
flags.DEFINE_list(
    "minBLA","4",
    "List of minBLA configuration for L1 fidelity calculation."
)

def get_func_by_task(task: str):
    """Get Processor class, model builder function, input builder function by task
    """
    if task == "spacev":
        from modeling.webSpaceV import model_fn_builder, file_based_input_fn_builder, eval_file_based_input_fn_builder
        return SpaceVProcessor(), model_fn_builder, file_based_input_fn_builder, eval_file_based_input_fn_builder
    elif task == "weight_attention":
        from modeling.webSpaceV_weight_attention import model_fn_builder, file_based_input_fn_builder, eval_file_based_input_fn_builder
        return SpaceVProcessor(), model_fn_builder, file_based_input_fn_builder, eval_file_based_input_fn_builder
    else:
        raise ValueError("Unsupported Task: " + task)


# this function will check how many example in a tfrecord file (used for eval-file)
# file_path must be str or list
def check_line_count_in_tfrecords(file_path):
    line_count = 0

    if isinstance(file_path, str):
        for _ in tf.python_io.tf_record_iterator(file_path):
            line_count += 1
    elif isinstance(file_path, list):
        for f in file_path:
            for _ in tf.python_io.tf_record_iterator(f):
                line_count += 1
    else:
        raise ValueError('file_path must be str or str-list')

    return line_count


# folder: folder path
# files: list
def find_all_file_in_folder(folder, file_paths):
    for file_name in os.listdir(folder):
        path = os.path.join(folder, file_name)

        if os.path.isfile(path):
            file_paths.append(path)
        elif os.path.isdir(path):
            find_all_file_in_folder(path, file_paths)


class ComputeFidelityHook(tf.train.SessionRunHook):
    def get_latest_global_steps(self, folder):
        max_global_step = -1

        for file_name in os.listdir(folder):
            path = os.path.join(folder, file_name)

            if os.path.isfile(path) and re.match('model.ckpt-[0-9]*\..*', file_name):
                prefix_size = len('model.ckpt-')

                step_str = file_name[prefix_size:len(file_name)].split('.')[0]

                step = int(step_str)

                if step > max_global_step:
                    max_global_step = step
        return max_global_step

    def __init__(self):
        '''
        output dir
        ----output_dir
            |
            ----saved_checkpoints
                |
                ----best_fidelity_info.txt (a float number)
                ----best_fidelity
        '''
        # a list store the fedility
        self.saved_checkpoints_path = os.path.join(FLAGS.output_dir, 'saved_checkpoints')
        self.best_fidelity_path = os.path.join(FLAGS.output_dir, 'saved_checkpoints', 'best_fidelity')
        self.best_fidelity_info_path = os.path.join(FLAGS.output_dir, 'saved_checkpoints', 'best_fidelity_info.txt')

        if os.path.exists(self.best_fidelity_info_path):
            fidelity_file_reader=open(self.best_fidelity_info_path,"r") 
            self.best_fidelity = float(fidelity_file_reader.readline())
            fidelity_file_reader.close()
        else:
            self.best_fidelity = 0.0
        # self.best_fidelity = 0.0

        # get tensorboard writer
        self.summary_writer = writer_cache.FileWriterCache.get(FLAGS.output_dir)

        # step
        max_global_step = self.get_latest_global_steps(FLAGS.output_dir)
        if max_global_step>0:
            self.fidelity_step = max_global_step/FLAGS.save_checkpoints_steps
        else:
            self.fidelity_step = 0

    '''loop check file in folder an get latest checkpoint file'''
    def get_latest_checkpoints_files(self, folder, expected_global_step):
        max_global_step = -1

        for file_name in os.listdir(folder):
            path = os.path.join(folder, file_name)

            if os.path.isfile(path) and re.match('model.ckpt-[0-9]*\..*', file_name):
                prefix_size = len('model.ckpt-')

                step_str = file_name[prefix_size:len(file_name)].split('.')[0]

                step = int(step_str)

                if step > max_global_step:
                    max_global_step = step
        
        if max_global_step != expected_global_step:
            tf.logging.warning('max_global_step is:%d is not equal expected_global_step:%d' % (max_global_step, expected_global_step))

        if max_global_step < 0:
            return []

        pattern = 'model.ckpt-' + str(max_global_step) + '\..*'

        ret = []

        for file_name in os.listdir(folder):
            path = os.path.join(folder, file_name)

            if os.path.isfile(path) and re.match(pattern, file_name):
                ret.append(path)

        return ret


    def end(self, session):
        tf.logging.info('Rank: %d, begin use compute_hvd to compute fidelity.' % (hvd.rank()))
        cur_fidelity = lf.compute_hvd(20)
        tf.logging.info('Rank: %d, get fidelity:%f.' % (hvd.rank(), cur_fidelity))

        if 0 == hvd.rank():
            expected_global_step = self.fidelity_step * FLAGS.save_checkpoints_steps

            tf.logging.info('Rank: %d, fidelity_step:%d, save_checkpoints_steps:%d, expected_global_step:%d.' 
                            % (hvd.rank(), self.fidelity_step, FLAGS.save_checkpoints_steps, expected_global_step))

            # write to tensorboard
            summary = tf.Summary()
            summary.value.add(tag="Fidelity", simple_value=cur_fidelity)
            self.summary_writer.add_summary(summary, expected_global_step)
            self.summary_writer.flush()

            tf.logging.info('Rank: %d, write to tensorboard.' % (hvd.rank()))

            if cur_fidelity > self.best_fidelity:
                # reset best fidelity
                tf.logging.info('Get a new best fidelity:%f' % (cur_fidelity))

                self.best_fidelity = cur_fidelity

                if os.path.exists(self.best_fidelity_info_path):
                    os.remove(self.best_fidelity_info_path)

                if not os.path.exists(self.best_fidelity_path):
                    os.makedirs(self.best_fidelity_path)
                
                best_fidelity_fp_writer = open(self.best_fidelity_info_path, 'w')
                best_fidelity_fp_writer.write(str(self.best_fidelity))
                best_fidelity_fp_writer.close()

                # copy best checkpoint to best_fidelity_path
                max_step_checkpoints_files = self.get_latest_checkpoints_files(FLAGS.output_dir, expected_global_step)

                if 0 == len(max_step_checkpoints_files):
                    tf.logging.warnings('can not find best checkpoints')
                else:
                    tf.logging.info('Rank: %d, wil copy files: %s to best_fidelity folder.' % (hvd.rank(), str(max_step_checkpoints_files)))

                    # remove old best fidelity
                    shutil.rmtree(self.best_fidelity_path) 

                    if not os.path.exists(self.best_fidelity_path):
                        os.makedirs(self.best_fidelity_path)

                    for f in max_step_checkpoints_files:
                        shutil.copy(f, os.path.join(self.best_fidelity_path, os.path.basename(f)))

                    tf.logging.info('save best fidelity checkpoint finish')
            else:
                tf.logging.info('Get new fidelity:%f, but best fidelity is:%f, will not save checkpoint.' % (cur_fidelity, self.best_fidelity))

        self.fidelity_step += 1


def main(_):
    tf.logging.set_verbosity(tf.logging.INFO)

    if FLAGS.horovod:
        hvd.init()

    if FLAGS.use_fp16:
        os.environ["TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE"] = "1"

    processor, model_fn_builder, file_based_input_fn_builder, eval_file_based_input_fn_builder = get_func_by_task(FLAGS.task_name.lower())

    # init l1fedility
    tf.logging.info("ideal_path path:%s", (FLAGS.ideal_path))
    lf.init(FLAGS.ideal_path)

    if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:
        raise ValueError(
            "At least one of `do_train`, `do_eval` or `do_predict' must be True.")

    query_bert_config = modeling_reuse_share_embedding_weight_attention.BertConfig.from_json_file(
        FLAGS.query_bert_config_file)
    meta_bert_config = modeling_reuse_share_embedding_weight_attention.BertConfig.from_json_file(
        FLAGS.meta_bert_config_file)

    all_meta_seq_length = FLAGS.max_seq_length_title+FLAGS.max_seq_length_anchor + \
        FLAGS.max_seq_length_url+FLAGS.max_seq_length_click

    if FLAGS.add_meta_description:
        all_meta_seq_length += FLAGS.max_seq_length_meta_description
    if FLAGS.add_language:
        all_meta_seq_length += 2
    if FLAGS.add_keyphrase:
        all_meta_seq_length += FLAGS.max_seq_length_keyphrase

    if FLAGS.max_seq_length_query > query_bert_config.max_position_embeddings:
        raise ValueError(
            "Cannot use query sequence length %d because the BERT model "
            "was only trained up to sequence length %d" %
            (FLAGS.max_seq_length_query, query_bert_config.max_position_embeddings))

    if all_meta_seq_length > meta_bert_config.max_position_embeddings:
        raise ValueError(
            "Cannot use meta sequence length %d because the BERT model "
            "was only trained up to sequence length %d" %
            (all_meta_seq_length, meta_bert_config.max_position_embeddings))

    tf.gfile.MakeDirs(FLAGS.output_dir)

    label_list = processor.get_labels()

    tokenizer = tokenization.FullTokenizerIDF(
        vocab_file=FLAGS.vocab_file,idf_file = FLAGS.idf_file, full_word_idf_file = FLAGS.full_word_idf_file,  do_lower_case=FLAGS.do_lower_case)
    
    tf.logging.info("Load vocab and idf file succeed!")
    master_process = True
    training_hooks = []
    global_batch_size = FLAGS.train_batch_size * FLAGS.num_accumulation_steps
    hvd_rank = 0

    config = tf.ConfigProto()
    if FLAGS.horovod:
        tf.logging.info("Multi-GPU training with TF Horovod")
        tf.logging.info("hvd.size() = %d hvd.rank() = %d",
                        hvd.size(), hvd.rank())

        global_batch_size = FLAGS.train_batch_size * \
            FLAGS.num_accumulation_steps * hvd.size()
        master_process = (hvd.rank() == 0)
        hvd_rank = hvd.rank()
        config.gpu_options.allow_growth = True
        config.gpu_options.visible_device_list = str(hvd.local_rank())
        if hvd.size() > 1:
            training_hooks.append(hvd.BroadcastGlobalVariablesHook(0))
    if FLAGS.use_xla:
        config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1

    run_config = tf.estimator.RunConfig(
        model_dir=FLAGS.output_dir if master_process else None,
        session_config=config,
        save_checkpoints_steps=FLAGS.save_checkpoints_steps if master_process else None,
        keep_checkpoint_max=3)

    if master_process:
        tf.logging.info("***** Configuaration *****")
        for key in FLAGS.__flags.keys():
            tf.logging.info('  {}: {}'.format(key, getattr(FLAGS, key)))
        tf.logging.info("**************************")

    train_examples = None
    num_train_steps = None
    num_warmup_steps = None
    train_examples_count = FLAGS.train_line_count
    log_train_run_hook = LogTrainRunHook(global_batch_size, hvd_rank)
    training_hooks.append(log_train_run_hook)

    if FLAGS.do_train:
        # train_examples = processor.get_train_examples(FLAGS.data_dir)
        # tmp_filenames = [os.path.join(FLAGS.output_dir, "train.tf_record")]
        num_train_steps = int(
            train_examples_count / global_batch_size * FLAGS.num_train_epochs)
        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)

    model_fn = model_fn_builder(
        task_name=FLAGS.task_name.lower(),
        query_bert_config=query_bert_config,
        meta_bert_config=meta_bert_config,
        num_labels=len(label_list),
        init_checkpoint=FLAGS.init_checkpoint,
        learning_rate=FLAGS.learning_rate,
        num_train_steps=num_train_steps,
        num_warmup_steps=num_warmup_steps,
        use_one_hot_embeddings=FLAGS.use_one_hot_embeddings,
        compressor_dim=FLAGS.compressor_dim,
        nce_temperature=FLAGS.nce_temperature,
        nce_weight=FLAGS.nce_weight,
        hvd=None if not FLAGS.horovod else hvd)

    estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        config=run_config)

    if FLAGS.do_train:
        start_index = 0
        end_index = FLAGS.train_partition_count

        if FLAGS.horovod:
            tfrecord_per_GPU = int(FLAGS.train_partition_count / hvd.size())
            start_index = hvd.rank() * tfrecord_per_GPU
            end_index = start_index+tfrecord_per_GPU

            if hvd.rank() == hvd.size():
                end_index = FLAGS.train_partition_count

        tf.logging.info("***** Running training *****")
        tf.logging.info("  Num examples = %d", train_examples_count)
        tf.logging.info("  Batch size = %d", FLAGS.train_batch_size)
        tf.logging.info("  Num steps = %d", num_train_steps)
        tf.logging.info("  hvd rank = %d", hvd.rank())
        tf.logging.info("  Num start_index = %d", start_index)
        tf.logging.info("  Num end_index = %d", end_index)

        train_file_list = []
        for i in range(start_index, end_index):
            train_file_list.append(os.path.join(
                FLAGS.preprocess_dir, str(i), FLAGS.preprocess_train_file_name))
        tf.logging.info("merge "+str(end_index-start_index) +
                        " preprocessed file from preprocess dir")
        tf.logging.info(train_file_list)

        train_input_fn = file_based_input_fn_builder(
            input_file=train_file_list,
            batch_size=FLAGS.train_batch_size,
            query_seq_length=FLAGS.max_seq_length_query,
            meta_seq_length=all_meta_seq_length,
            is_training=True,
            drop_remainder=True,
            is_fidelity_eval=False,
            hvd=None if not FLAGS.horovod else hvd)

        # initilize eval file
        # must set preprocess_eval_dir, all file in folder preprocess_eval_dir will thinked as tfrecord file
        if FLAGS.preprocess_eval_dir is None:
            raise ValueError('must set preprocess_eval_dir by hand.')

        all_eval_files = []
        eval_file_list = []

        find_all_file_in_folder(FLAGS.preprocess_eval_dir, all_eval_files)

        for i in range(len(all_eval_files)):
            if hvd.rank() == i % hvd.size():
                eval_file_list.append(all_eval_files[i])

        if 0 == len(eval_file_list):
            raise ValueError('  Rank: %d get eval file empty.' % (hvd.rank()))
        
        tf.logging.info("**********Check how many eval example in current rank*************")
        eval_examples_count = check_line_count_in_tfrecords(eval_file_list)

        eval_steps = int(math.ceil(eval_examples_count / FLAGS.eval_batch_size))

        tf.logging.info("***** Running evaluation *****")
        tf.logging.info("  Rank: %d will eval files:%s" %
                        (hvd.rank(), str(eval_file_list)))
        tf.logging.info("  Rank: %d eval example count:%d" %
                        (hvd.rank(), eval_examples_count))
        tf.logging.info("  Rank: %d eval batch size:%d" %
                        (hvd.rank(), FLAGS.eval_batch_size))
        tf.logging.info("  Rank: %d eval_steps:%d" %
                        (hvd.rank(), eval_steps))

        eval_input_fn = eval_file_based_input_fn_builder(
            input_file=eval_file_list,
            query_seq_length=FLAGS.max_seq_length_query,
            meta_seq_length=all_meta_seq_length,
            drop_remainder=False,
            is_fidelity_eval=True)

        # create InMemoryEvaluatorHook 
        fidelity_evaluator = tf.estimator.experimental.InMemoryEvaluatorHook(
            estimator=estimator,
            steps=eval_steps, # steps must be set or will not print any log, do not know why
            input_fn=eval_input_fn,
            every_n_iter=FLAGS.save_checkpoints_steps,
            hooks=[ComputeFidelityHook()],
            name="fidelity_eval")

        training_hooks.append(fidelity_evaluator)

        train_start_time = time.time()

        estimator.train(input_fn=train_input_fn,
                        max_steps=num_train_steps, hooks=training_hooks)

        train_time_elapsed = time.time() - train_start_time
        train_time_wo_overhead = log_train_run_hook.total_time
        avg_sentences_per_second = num_train_steps * \
            global_batch_size * 1.0 / train_time_elapsed
        ss_sentences_per_second = (
            num_train_steps - log_train_run_hook.skipped) * global_batch_size * 1.0 / train_time_wo_overhead

        if master_process:
            tf.logging.info("-----------------------------")
            tf.logging.info("Total Training Time = %0.2f for Sentences = %d", train_time_elapsed,
                            num_train_steps * global_batch_size)
            tf.logging.info("Total Training Time W/O Overhead = %0.2f for Sentences = %d", train_time_wo_overhead,
                            (num_train_steps - log_train_run_hook.skipped) * global_batch_size)
            tf.logging.info(
                "Throughput Average (sentences/sec) with overhead = %0.2f", avg_sentences_per_second)
            tf.logging.info(
                "Throughput Average (sentences/sec) = %0.2f", ss_sentences_per_second)
            tf.logging.info("-----------------------------")

    if FLAGS.do_eval and master_process:
        eval_examples, raw_lines = processor.get_dev_examples(FLAGS.data_dir, FLAGS.eval_file, FLAGS.src_col, FLAGS.title_col,
                                                              FLAGS.anchor_col, FLAGS.url_col, FLAGS.click_col, FLAGS.label_col)
        eval_file = os.path.join(FLAGS.output_dir, "eval.tf_record")
        file_based_convert_spaceV_examples_to_features(
            eval_examples, label_list, FLAGS.max_seq_length_query, FLAGS.max_seq_length_title, FLAGS.max_seq_length_anchor,
            FLAGS.max_seq_length_url, FLAGS.max_seq_length_click, tokenizer, eval_file)

        tf.logging.info("***** Running evaluation *****")
        tf.logging.info("  Num examples = %d", len(eval_examples))
        tf.logging.info("  Batch size = %d", FLAGS.eval_batch_size)

        eval_drop_remainder = False
        eval_input_fn = file_based_input_fn_builder(
            input_file=eval_file,
            batch_size=FLAGS.eval_batch_size,
            query_seq_length=FLAGS.max_seq_length_query,
            meta_seq_length=all_meta_seq_length,
            is_training=False,
            drop_remainder=eval_drop_remainder)

        eval_hooks = [LogEvalRunHook(FLAGS.eval_batch_size)]
        eval_start_time = time.time()
        result = estimator.evaluate(input_fn=eval_input_fn, hooks=eval_hooks)

        eval_time_elapsed = time.time() - eval_start_time
        eval_time_wo_overhead = eval_hooks[-1].total_time

        time_list = eval_hooks[-1].time_list
        time_list.sort()
        num_sentences = (
            eval_hooks[-1].count - eval_hooks[-1].skipped) * FLAGS.eval_batch_size

        avg = np.mean(time_list)
        cf_50 = max(time_list[:int(len(time_list) * 0.50)])
        cf_90 = max(time_list[:int(len(time_list) * 0.90)])
        cf_95 = max(time_list[:int(len(time_list) * 0.95)])
        cf_99 = max(time_list[:int(len(time_list) * 0.99)])
        cf_100 = max(time_list[:int(len(time_list) * 1)])
        ss_sentences_per_second = num_sentences * 1.0 / eval_time_wo_overhead

        tf.logging.info("-----------------------------")
        tf.logging.info("Total Inference Time = %0.2f for Sentences = %d", eval_time_elapsed,
                        eval_hooks[-1].count * FLAGS.eval_batch_size)
        tf.logging.info("Total Inference Time W/O Overhead = %0.2f for Sentences = %d", eval_time_wo_overhead,
                        (eval_hooks[-1].count - eval_hooks[-1].skipped) * FLAGS.eval_batch_size)
        tf.logging.info("Summary Inference Statistics on EVAL set")
        tf.logging.info("Batch size = %d", FLAGS.eval_batch_size)
        tf.logging.info("Precision = %s", "fp16" if FLAGS.use_fp16 else "fp32")
        tf.logging.info(
            "Latency Confidence Level 50 (ms) = %0.2f", cf_50 * 1000)
        tf.logging.info(
            "Latency Confidence Level 90 (ms) = %0.2f", cf_90 * 1000)
        tf.logging.info(
            "Latency Confidence Level 95 (ms) = %0.2f", cf_95 * 1000)
        tf.logging.info(
            "Latency Confidence Level 99 (ms) = %0.2f", cf_99 * 1000)
        tf.logging.info(
            "Latency Confidence Level 100 (ms) = %0.2f", cf_100 * 1000)
        tf.logging.info("Latency Average (ms) = %0.2f", avg * 1000)
        tf.logging.info(
            "Throughput Average (sentences/sec) = %0.2f", ss_sentences_per_second)
        tf.logging.info("-----------------------------")

        output_eval_file = os.path.join(FLAGS.output_dir, "eval_results.txt")
        with tf.gfile.GFile(output_eval_file, "w") as writer:
            tf.logging.info("***** Eval results *****")
            for key in sorted(result.keys()):
                tf.logging.info("  %s = %s", key, str(result[key]))
                writer.write("%s = %s\n" % (key, str(result[key])))

    if FLAGS.do_predict and master_process:
        predict_examples, raw_lines = processor.get_test_examples(FLAGS.data_dir, FLAGS.predict_file, FLAGS.src_col, FLAGS.title_col,
                                                                  FLAGS.anchor_col, FLAGS.url_col, FLAGS.click_col, FLAGS.label_col)
        if FLAGS.preprocess_predict_dir is None:
            tf.logging.info("Predict dir is needed for prediction mode!")
        all_predict_files = []
        find_all_file_in_folder(FLAGS.preprocess_predict_dir, all_predict_files)

        tf.logging.info("***** Running prediction*****")
        # tf.logging.info("  Num examples = %d", len(predict_examples))
        predict_drop_remainder = False
        predict_input_fn = file_based_input_fn_builder(
            input_file=all_predict_files,
            batch_size=FLAGS.predict_batch_size,
            query_seq_length=FLAGS.max_seq_length_query,
            meta_seq_length=all_meta_seq_length,
            is_training=False,
            drop_remainder=predict_drop_remainder)

        predict_hooks = [LogEvalRunHook(FLAGS.predict_batch_size)]
        predict_start_time = time.time()

        output_predict_file = os.path.join(
            FLAGS.output_dir, "predict_test2.tsv")
        result = estimator.predict(
            input_fn=predict_input_fn, hooks=predict_hooks)
        count = 0
        with tf.gfile.GFile(output_predict_file, "w") as writer:
            tf.logging.info("***** Predict results *****")
            # for (prediction,example) in zip(result,raw_lines):
            #           count += 1
            #           output_line = '\t'.join(str(item) for item in example) +'\t' + '|'.join(str(class_probability) for class_probability in prediction[0])+'\t' + '|'.join(str(class_probability) for class_probability in prediction[1]) + '\n'
            #           writer.write(output_line)
            for (prediction, example) in zip(result, raw_lines):
                count += 1
                output_line = '\t'.join(str(item) for item in example) + '\t' + '\t'.join(
                    str(class_probability) for class_probability in prediction) + '\n'
                writer.write(output_line)

        tf.logging.info("Prediction is done!")


if __name__ == "__main__":
    flags.mark_flag_as_required("data_dir")
    flags.mark_flag_as_required("task_name")
    flags.mark_flag_as_required("vocab_file")
    flags.mark_flag_as_required("output_dir")
    flags.mark_flag_as_required("idf_file")
    flags.mark_flag_as_required("full_word_idf_file")
    tf.app.run()
